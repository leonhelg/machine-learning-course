{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('Conda-TF-Nightly-v2.4.0-GPU': conda)",
   "display_name": "Python 3.7.9 64-bit ('Conda-TF-Nightly-v2.4.0-GPU': conda)",
   "metadata": {
    "interpreter": {
     "hash": "632eda4776f98d4df847e2adfe21c25da6fe8f6364747f542a685eecb742cdde"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "UC3MAL201 Machine Learning Session 10 Tutorial 10 Leon Eriksen Helgeland -\n",
    "First written 05.10.2020\n",
    "\n",
    "# Neural Network from Scratch\n",
    "\n",
    "1. Network intialization\n",
    "2. Forward propgation of inputs\n",
    "3. Back propogation of errors\n",
    "4. Training the Network\n",
    "5. Making predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random, seed, randrange\n",
    "seed(42)"
   ]
  },
  {
   "source": [
    "## Dataset pre-processing\n",
    "## a. Reading the dataset\n",
    "Paths to the dataset:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Paths to the dataset\n",
    "dataset_file = r'seeds_dataset.csv'\n",
    "dataset_directory = r'C:\\Users\\leon.helgeland\\Documents\\GitHub\\machine-learning-course\\Session-10-neural-network-from-scratch\\dataset' \n",
    "#'~/GitHub/BigData/dataset11'\n",
    "path = f'{dataset_directory}\\{dataset_file}'\n",
    "path = 'seeds_dataset.csv'"
   ]
  },
  {
   "source": [
    "This function reads the csv file in a dictionary and converts it to a two dimensional list."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def read_float_csv(path):\n",
    "    '''\n",
    "    This function reads the csv file in a dictionary and converts it to a two dimensional list.\n",
    "    '''\n",
    "    with open(path,\"r\") as f:\n",
    "        return [[float(i) for i in line.split(',')] for line in f] # Loops through lines, splits on comma, adds values to list as float(which removes '\\n')."
   ]
  },
  {
   "source": [
    "Instantiating the read_csv:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 998 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = read_float_csv(path)\n",
    "df_org = df"
   ]
  },
  {
   "source": [
    "## b. Normalize independant values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A mess that works (too late in the evening now)\n",
    "normalized_value = lambda a,b,c:(a-b)/(c-b)\n",
    "maxmin_ = [[max([item[j] for item in df]), min([item[j] for item in df])] for j in range(len(df[0])-1)]\n",
    "l1 = [[normalized_value(item[j],maxmin_[j][1],maxmin_[j][0]) for item in df] for j in range(len(df[0])-1)]\n",
    "l2 = []\n",
    "for i in range(len(l1[0])):  \n",
    "    row =[] \n",
    "    for item in l1: \n",
    "        row.append(item[i]) \n",
    "    l2.append(row)\n",
    "#for i in range(0,210):\n",
    "#    l2[i].append(df[i][7])\n",
    "df = l2"
   ]
  },
  {
   "source": [
    "## c. Hot one encode dependant column"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Also a mess that works (even later in the evening now, been at school for 13 hours)\n",
    "column_ = []\n",
    "for i in range(0,210):\n",
    "    column_.append(df_org[i][7])\n",
    "new_columns = []\n",
    "for i in range(0,210):\n",
    "    new_columns.append([])\n",
    "    if column_[i] == 1:\n",
    "        new_columns[i].append(1)\n",
    "        new_columns[i].append(0)\n",
    "        new_columns[i].append(0)\n",
    "    elif column_[i] == 2:\n",
    "        new_columns[i].append(0)\n",
    "        new_columns[i].append(1)\n",
    "        new_columns[i].append(0)\n",
    "    elif column_[i] == 3:\n",
    "        new_columns[i].append(0)\n",
    "        new_columns[i].append(0)\n",
    "        new_columns[i].append(1)\n",
    "#print(new_columns)\n",
    "#for i in range(0,210):\n",
    "#    df[i].append(new_columns[i][0])\n",
    "#    df[i].append(new_columns[i][1])\n",
    "#    df[i].append(new_columns[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = df\n",
    "y = new_columns"
   ]
  },
  {
   "source": [
    "## d. Train test split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def train_test_split(X_in,y_in,ratio_value_0):\n",
    "    '''\n",
    "    This function takes X and y as input and splits them into a training and test set depending on the ratio value. The input ratio value decides the first of the two ratio values. Ex. to get a split of 8:2 the ration value should be 8. Else 7 would become a ratio of 7:3. \n",
    "    '''\n",
    "    train_X = []\n",
    "    train_y = []\n",
    "    X_temp = X_in\n",
    "    y_temp = y_in\n",
    "    train_length = int((len(X_temp)/10)*ratio_value_0)\n",
    "    test_length = train_length-len(X_temp)\n",
    "    counter = train_length\n",
    "    #print(len(X_temp),ratio_value_0)\n",
    "    while counter > 0:\n",
    "        #print(counter)\n",
    "        train_X.append(X_temp.pop(randrange(0,counter)))\n",
    "        train_y.append(y_temp.pop(randrange(0,counter)))\n",
    "        counter-=1\n",
    "    test_X = X_temp\n",
    "    test_y = y_temp\n",
    "\n",
    "    return train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = train_test_split(X,y,8)"
   ]
  },
  {
   "source": [
    "## 1. Network initialization\n",
    "This function intializes a two layer neural network with random wights on the edges."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def initialize_network(n_inputs,n_hidden,n_outputs):\n",
    "    '''\n",
    "    Intializes a two layer neural network with random wights on the edges.\n",
    "    '''\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)] # Adds random start weighting values to the layer.\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)] # +1 is the bias added to the layer (important to shift the weights in the transform function away from 0)\n",
    "    network.append(output_layer)\n",
    "    return network"
   ]
  },
  {
   "source": [
    "Output:\n",
    "- 0: [{'weights': [0.6394267984578837, 0.025010755222666936, 0.27502931836911926]}]\n",
    "- 1: [{'weights': [0.22321073814882275, 0.7364712141640124]}, {'weights': [0.6766994874229113, 0.8921795677048454]}]\n",
    "\n",
    "Explenation:\n",
    "- 0: 1 = First connection section weight from input layer to hidden layer. 2 = Second connection section weight from input layer to hidden layer. 3 = The bias added for the hidden nodes.\n",
    "- 1: 1 = First connection section weight from hidden layer to output layer. 2 = The bias for the node. 3 = Second connection section weight from hidden layer to output layer. 4 = The bias for the node."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. Forward propogation of inputs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def activate(weights, inputs):\n",
    "    '''\n",
    "    This function represents the internal function at work inside the artificical neuron.\n",
    "    The main purpose is to sum up weights multiplied with the inputs which will then run through a transfer function to get the neuron's (local)output.\n",
    "    or \"Calculate neuron activation for an input\"\n",
    "    '''\n",
    "    activation = weights[-1] # Get the current selected bias and start with that when summarizing the (local)input and weights. We use bias so we dont start with 0.\n",
    "    for i in range(len(weights)-1): # Loops through all elements execept bias.\n",
    "        activation += weights[i] + inputs[i] # Gives a summazion of all the inputs coming to the node. (Creates the actual important node value).\n",
    "    return activation # Returns the sum of the above lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from math import exp\n",
    "def transfer_sigmoid(activation):\n",
    "    '''\n",
    "    This function transfers the neuron activation using the sigmoid function. This is an essentail operation inside the artificial neuron.\n",
    "    '''\n",
    "    return 1.0 / (1.0 + exp(-activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def forward_propogate(network, row):\n",
    "    '''\n",
    "    This function forward propagates (global)input to the hidden layer an applies the activation and the transformation to create the new (local)inputs for the network output layer.\n",
    "    x(input) ==> Activate : calculate(bias, weights and input values) ==> Transform : Sigmoid ==> result is input for output layer.\n",
    "    '''\n",
    "    inputs = row # Goes through each row in the dataframe\n",
    "    for layer in network: # Loops through all layers.\n",
    "        new_inputs = [] # Define a list to collect new neuron output.\n",
    "        for neuron in layer: # Looops through all neurons in each layer.\n",
    "            activation = activate(neuron['weights'], inputs) # Runs the activation function using inputs and weights on the neuron in selection.\n",
    "            neuron['output'] = transfer_sigmoid(activation) # Then the transfer function is applied to the activation function results. The result is written to a list.\n",
    "            new_inputs.append(neuron['output']) # Collects the new output-layer inputs.\n",
    "        inputs = new_inputs # Writes the new outputs/inputs depend how you see it, but its towards the next layer, or in this case new (local)inputs to the output layer.\n",
    "    return inputs # Returns the new output-layer inputs"
   ]
  },
  {
   "source": [
    "## 3. Back propogation of errors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def transfer_derivative(output):\n",
    "    '''\n",
    "    This function cauclates the derivative of the input to the output in regards to the output of the output.\n",
    "    '''\n",
    "    return output * (1.0 - output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def backward_propagate_error(network, expected):\n",
    "    '''\n",
    "    This function calculates the back propagation using the difference between the actual output and the expected output. + some maths\n",
    "    '''\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i] # Backwards looping through the network layers\n",
    "        errors = list() # For collecting errors\n",
    "        if i != len(network)-1: # If its not the output layer\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta']) # Calculates the error for each weight or edge. Using the delta from the neuron.\n",
    "                    errors.append(error)\n",
    "        else: # This is the part that works with the output layer\n",
    "            for j in range(len(layer)): # Loops through the amount of neurons in the layer\n",
    "                neuron = layer[j] # Select neuron using the amount above\n",
    "                errors.append(expected[j] - neuron['output']) # The error is the difference between the expected output and the actual output.\n",
    "        for j in range(len(layer)): # Loops through the amount of neurons in the layer\n",
    "            neuron = layer[j] # Select neuron using the amount above\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output']) # ehm. ?Error linked to all nodes we are considering in the layer? # How much does this node contribute as a percentage to the totalt error of the output."
   ]
  },
  {
   "source": [
    "## 4. Training the Network\n",
    "Update the weigths (we need the learning rate)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def update_weights(network, row, l_rate): # row\n",
    "    '''\n",
    "    This function uses the errors to update the weights in the edges between neurons to improve the output result.\n",
    "    '''\n",
    "    for i in range(len(network)): # Loops through the layer in the network\n",
    "        inputs = row # This: [:-1] is only used when a column is target classes \n",
    "        if i != 0: # For the first layer the inputs are the network inputs. For all other layer, the input is the last layer.\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]] # Selecting the input to be the output of the last layer.\n",
    "        for neuron in network[i]: # looping through neurons in the active layer i\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j] # The actual weight update for each edge for each layer, learning rate multiplied by delta and input\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta'] # Updating the bias, since its also contributing to the output error. You dont need this if you dont have a bias. Does not have a input, becase its not a node itself, just a number that is added to the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def train_network(network, train_x, train_Y, l_rate, n_epoch, n_outputs):\n",
    "    '''\n",
    "    This function calls all the other functions we made in a way that ensures that the whole network functions properly. It uses a fixed number of epochs. It's using online learning, because its only presented with one case at the time and based on that we update the weight. (Its also possible to do batch training giving a batch of cases at once which will be more effective. Refer to lecture for answer.)\n",
    "    '''\n",
    "    for epoch in range(n_epoch): # Goes through training for each specificed epoch\n",
    "        sum_error = 0 # Makes a counter that sums up the total errors of all the outputs\n",
    "        for i in range(len(train_X)): # For each row or event/case in the training data.\n",
    "            row = train_x[i]\n",
    "            outputs = forward_propogate(network, row) # Start forward propagating through the random values created by the intialization and then you are left with the output\n",
    "            expected = [0 for i in range(n_outputs)] # a\n",
    "            expected[train_Y] = 1 # b ##### This does not work since there is one column in the original dataset this algorithm was built upon but three columns in the new dataset i will be using.\n",
    "            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))]) # c Calculates the error based on the expected vs actual output\n",
    "            backward_propagate_error(network, expected) # Backpropagation using the expected values to determine the error\n",
    "            update_weights(network, row, l_rate) # Update the weights and make it ready for next element.\n",
    "        print('>epoch=%d, error=%.3f' % (epoch, sum_error))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train the network\n",
    "n_inputs = len(X_train[0])\n",
    "n_outputs = len(y_train[0])\n",
    "network = initialize_network(n_inputs, 2, n_outputs)\n",
    "train_network(network, )"
   ]
  },
  {
   "source": [
    "## 5. Making predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def predict(network, row):\n",
    "    '''\n",
    "    This function is made to test a prediction on a already trained network. How it does that is by just using forward propagation not backwards propagation.\n",
    "    '''\n",
    "    outputs = forward_propogate(network, row)\n",
    "    return outputs.index(max(outputs))"
   ]
  },
  {
   "source": [
    "# Testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#seed(42)\n",
    "\n",
    "# network = initialize_network(7,1,3)\n",
    "\n",
    "# Test network\n",
    "#network = initialize_network(2,1,2) # Input, hidden, output\n",
    "#for index, layer in enumerate(network):\n",
    "#    print(f'{index}: {layer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "# Intialize\n",
    "\n",
    "#row = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#normalized_value = lambda a,b,c:(a-b)/(c-b)\n",
    "#print(normalized_value(21.17,10.59,21.18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#x = 21.17\n",
    "#x_min = 10.59\n",
    "#x_max = 21.18\n",
    "#x_new = (x - x_min) / (x_max - x_min)\n",
    "#print(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#def train_test_split(y_column_index = data):\n",
    "    # if y != 0 or 1:\n",
    "    # then hot one encoder\n",
    "#    return X_train = [], y_train = [], X_test = [], y_test = []\n",
    "\n",
    "#def model_fit(X_train, y_train, config):"
   ]
  }
 ]
}