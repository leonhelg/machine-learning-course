{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('Conda-TF-Nightly-v2.4.0-GPU': conda)",
   "display_name": "Python 3.7.9 64-bit ('Conda-TF-Nightly-v2.4.0-GPU': conda)",
   "metadata": {
    "interpreter": {
     "hash": "632eda4776f98d4df847e2adfe21c25da6fe8f6364747f542a685eecb742cdde"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "UC3MAL201 Machine Learning Session 10 Tutorial 10 Leon Eriksen Helgeland -\n",
    "First written 05.10.2020\n",
    "\n",
    "# Neural Network from Scratch\n",
    "\n",
    "1. Network intialization\n",
    "2. Forward propgation of inputs\n",
    "3. Back propogation of errors\n",
    "4. Training the Network\n",
    "5. Making predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Dataset pre-processing\n",
    "## a. Reading the dataset\n",
    "Paths to the dataset:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 0 ns\n"
    }
   ],
   "source": [
    "%%time\n",
    "# Paths to the dataset\n",
    "dataset_file = r'seeds_dataset.csv'\n",
    "dataset_directory = r'C:\\Users\\leon.helgeland\\Documents\\GitHub\\machine-learning-course\\Session-10-neural-network-from-scratch\\dataset' \n",
    "#'~/GitHub/BigData/dataset11'\n",
    "path = f'{dataset_directory}\\{dataset_file}'\n",
    "path = 'seeds_dataset.csv'"
   ]
  },
  {
   "source": [
    "This function reads the csv file in a dictionary and converts it to a two dimensional list."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 0 ns\n"
    }
   ],
   "source": [
    "%%time\n",
    "def read_float_csv(path):\n",
    "    '''\n",
    "    This function reads the csv file in a dictionary and converts it to a two dimensional list.\n",
    "    '''\n",
    "    with open(path,\"r\") as f:\n",
    "        return [[float(i) for i in line.split(',')] for line in f] # Loops through lines, splits on comma, adds values to list as float(which removes '\\n')."
   ]
  },
  {
   "source": [
    "Instantiating the read_csv:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 997 Âµs\n"
    }
   ],
   "source": [
    "%%time\n",
    "df = read_float_csv(path)\n",
    "df_org = df"
   ]
  },
  {
   "source": [
    "## b. Normalize independant values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A mess that works (too late in the evening now)\n",
    "normalized_value = lambda a,b,c:(a-b)/(c-b)\n",
    "maxmin_ = [[max([item[j] for item in df]), min([item[j] for item in df])] for j in range(len(df[0])-1)]\n",
    "l1 = [[normalized_value(item[j],maxmin_[j][1],maxmin_[j][0]) for item in df] for j in range(len(df[0])-1)]\n",
    "l2 = []\n",
    "for i in range(len(l1[0])):  \n",
    "    row =[] \n",
    "    for item in l1: \n",
    "        row.append(item[i]) \n",
    "    l2.append(row)\n",
    "#for i in range(0,210):\n",
    "#    l2[i].append(df[i][7])\n",
    "df = l2"
   ]
  },
  {
   "source": [
    "## c. Hot one encode dependant column"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Also a mess that works (even later in the evening now, been at school for 13 hours)\n",
    "column_ = []\n",
    "for i in range(0,210):\n",
    "    column_.append(df_org[i][7])\n",
    "new_columns = []\n",
    "for i in range(0,210):\n",
    "    new_columns.append([])\n",
    "    if column_[i] == 1:\n",
    "        new_columns[i].append(1)\n",
    "        new_columns[i].append(0)\n",
    "        new_columns[i].append(0)\n",
    "    elif column_[i] == 2:\n",
    "        new_columns[i].append(0)\n",
    "        new_columns[i].append(1)\n",
    "        new_columns[i].append(0)\n",
    "    elif column_[i] == 3:\n",
    "        new_columns[i].append(0)\n",
    "        new_columns[i].append(0)\n",
    "        new_columns[i].append(1)\n",
    "#print(new_columns)\n",
    "for i in range(0,210):\n",
    "    df[i].append(new_columns[i][0])\n",
    "    df[i].append(new_columns[i][1])\n",
    "    df[i].append(new_columns[i][2])"
   ]
  },
  {
   "source": [
    "## 1. Network initialization\n",
    "This function intializes a two layer neural network with random wights on the edges."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from random import random, seed\n",
    "\n",
    "def initialize_network(n_inputs,n_hidden,n_outputs):\n",
    "    '''\n",
    "    Intializes a two layer neural network with random wights on the edges.\n",
    "    '''\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)] # Adds random start weighting values to the layer.\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)] # +1 is the bias added to the layer (important to shift the weights in the transform function away from 0)\n",
    "    network.append(output_layer)\n",
    "    return network"
   ]
  },
  {
   "source": [
    "Intializing the network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "seed(42)\n",
    "\n",
    "network = initialize_network(7,1,3)\n",
    "\n",
    "# Test network\n",
    "#network = initialize_network(2,1,2) # Input, hidden, output\n",
    "#for index, layer in enumerate(network):\n",
    "#    print(f'{index}: {layer}')"
   ]
  },
  {
   "source": [
    "Output:\n",
    "- 0: [{'weights': [0.6394267984578837, 0.025010755222666936, 0.27502931836911926]}]\n",
    "- 1: [{'weights': [0.22321073814882275, 0.7364712141640124]}, {'weights': [0.6766994874229113, 0.8921795677048454]}]\n",
    "\n",
    "Explenation:\n",
    "- 0: 1 = First connection section weight from input layer to hidden layer. 2 = Second connection section weight from input layer to hidden layer. 3 = The bias added for the hidden nodes.\n",
    "- 1: 1 = First connection section weight from hidden layer to output layer. 2 = The bias for the node. 3 = Second connection section weight from hidden layer to output layer. 4 = The bias for the node."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. Forward propogation of inputs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def activate(weights, inputs):\n",
    "    '''\n",
    "    This function represents the internal function at work inside the artificical neuron.\n",
    "    The main purpose is to sum up weights multiplied with the inputs which will then run through a transfer function to get the neuron's (local)output.\n",
    "    or \"Calculate neuron activation for an input\"\n",
    "    '''\n",
    "    activation = weights[-1] # Get the current selected bias and start with that when summarizing the (local)input and weights. We use bias so we dont start with 0.\n",
    "    for i in range(len(weights)-1): # Loops through all elements execept bias.\n",
    "        activation + = weights[i] + inputs[i] # Gives a summazion of all the inputs coming to the node. (Creates the actual important node value).\n",
    "    return activation # Returns the sum of the above lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from math import exp\n",
    "def transfer_sigmoid(activation):\n",
    "    '''\n",
    "    This function transfers the neuron activation using the sigmoid function. This is an essentail operation inside the artificial neuron.\n",
    "    '''\n",
    "    return 1.0 / (1.0 + exp(-activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def forward_propogate(network, row):\n",
    "    '''\n",
    "    This function forward propagates (global)input to the hidden layer an applies the activation and the transformation to create the new (local)inputs for the network output layer.\n",
    "    x(input) ==> Activate : calculate(bias, weights and input values) ==> Transform : Sigmoid ==> result is input for output layer.\n",
    "    '''\n",
    "    inputs = row # Goes through each row in the dataframe\n",
    "    for layer in network: # Loops through all layers.\n",
    "        new_inputs = [] # Define a list to collect new neuron output.\n",
    "        for neuron in layer: # Looops through all neurons in each layer.\n",
    "            activation = activation(neuron['weights'], inputs) # Runs the activation function using inputs and weights on the neuron in selection.\n",
    "            neuron['output'] = transfer_sigmoid(activation) # Then the transfer function is applied to the activation function results. The result is written to a list.\n",
    "            new_inputs.append(neuron['output']) # Collects the new output-layer inputs.\n",
    "        inputs = new_inputs # Writes the new outputs/inputs depend how you see it, but its towards the next layer, or in this case new (local)inputs to the output layer.\n",
    "    return inputs # Returns the new output-layer inputs"
   ]
  },
  {
   "source": [
    "## 3. Back propogation of errors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_derivative(output):\n",
    "    '''\n",
    "    This function\n",
    "    '''\n",
    "    return output * (1.0 - output)"
   ]
  },
  {
   "source": [
    "# Testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Intialize\n",
    "\n",
    "row = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalized_value = lambda a,b,c:(a-b)/(c-b)\n",
    "print(normalized_value(21.17,10.59,21.18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = 21.17\n",
    "x_min = 10.59\n",
    "x_max = 21.18\n",
    "x_new = (x - x_min) / (x_max - x_min)\n",
    "print(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def train_test_split(y_column_index = data):\n",
    "    # if y != 0 or 1:\n",
    "    # then hot one encoder\n",
    "    return X_train = [], y_train = [], X_test = [], y_test = []\n",
    "\n",
    "def model_fit(X_train, y_train, config):\n"
   ]
  }
 ]
}