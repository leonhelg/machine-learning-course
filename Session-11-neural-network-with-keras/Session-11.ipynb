{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('Conda-TF-Nightly-v2.4.0-GPU': conda)",
   "display_name": "Python 3.7.9 64-bit ('Conda-TF-Nightly-v2.4.0-GPU': conda)",
   "metadata": {
    "interpreter": {
     "hash": "632eda4776f98d4df847e2adfe21c25da6fe8f6364747f542a685eecb742cdde"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UC3MAL201 Machine Learning Session 11 Tutorial 11 Leon Eriksen Helgeland -\n",
    "First written 09.09.2020\n",
    "\n",
    "# Neural Network - with Keras"
   ]
  },
  {
   "source": [
    "## 1. Dataset description"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- Name of the datset: MNIST\n",
    "- Contents:     28x28 pixel grayscale images\n",
    "- Images contain: Handwritten single digits, 0-9\n",
    "- Total dataset length: 70,000 images. 60,000 train-set and 10,000 test-set \n",
    "- Total dataset row length: 785 columns.\n",
    "- First column: 1 class column, digit 0-9.\n",
    "- All other columns : 784 feature columns, pixels from 0 to 255"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. Data pre-processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train = pd.read_csv(r'C:\\Users\\leon.helgeland\\OneDrive\\S05-MAL-2020\\S05MAL - Session 11 - Neural Network with Keras - Live Demo\\Dataset for the tutorial-20200908\\mnist_train.csv',header=None)\n",
    "test = pd.read_csv(r'C:\\Users\\leon.helgeland\\OneDrive\\S05-MAL-2020\\S05MAL - Session 11 - Neural Network with Keras - Live Demo\\Dataset for the tutorial-20200908\\mnist_test.csv',header=None)\n",
    "X_test = test.iloc[:,1:].apply(lambda a: a*(0.99/255)+0.01) # Rescaling 0-255 pixel values to 0.01-1\n",
    "X_train = train.iloc[:,1:].apply(lambda a: a*(0.99/255)+0.01) # Rescaling 0-255 pixel values to 0.01-1\n",
    "# Above: This rescaling process helps to prevent having 0 values as inputs which can cause weight update problems during network synthesis.\n",
    "\n",
    "enc = OneHotEncoder(categories=\"auto\", sparse=False, dtype=np.int16) # Removed: handle_unknown='ignore'\n",
    "y_train = enc.fit_transform(train.iloc[:,:1].to_numpy().reshape(-1, 1)) # OneHotEncoding single column to binary columns\n",
    "y_test = enc.fit_transform(test.iloc[:,:1].to_numpy().reshape(-1, 1)) # OneHotEncoding single column to binary columns"
   ]
  },
  {
   "source": [
    "## 3. Modelling process"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def model_fit(X_train, y_train, config):\n",
    "    n_input = config[0]\n",
    "    n_output = config[1]\n",
    "    n_nodes = config[2]\n",
    "    epoch = config[3]\n",
    "    batch = config[4]\n",
    "    # Define and Compile\n",
    "    model = Sequential() # Model activation\n",
    "    model.add(Dense(n_nodes, input_dim=n_input, activation='relu')) # Input layer with 'relu' function\n",
    "    # Test adding these layers:\n",
    "    # - Conv + Pool x2\n",
    "    # - Conv x2 (optional)\n",
    "    # - Conv + Pool x1 (optional)\n",
    "    # - Full x2\n",
    "    model.add(Dense(n_output, activation='softmax')) # Output layer with 'softmax' function \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # Compling the output \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train, epochs=epoch, batch_size=batch)\n",
    "    return model"
   ]
  },
  {
   "source": [
    "## 4. Prediction process"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/')\n",
    "def rescale_image(a)\n",
    "    '''\n",
    "    Takes a image value between 0-255 as input and converts it to a value between 0.01 and 1.\n",
    "    '''\n",
    "    return lambda a : a*(0,99/255)+0.01\n",
    "df[:-1] = [[rescale_image(pixel) for pixel in row] for row in df[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow.keras.optimizers\n",
    "import os\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset_train_file = 'mnist_train.csv'\n",
    "dataset_test_file = 'mnist_test.csv'\n",
    "dataset_directory = r'C:\\Users\\leon.helgeland\\OneDrive\\S05-MAL-2020\\S05MAL - Session 11 - Neural Network with Keras - Live Demo\\Dataset for the tutorial-20200908'\n",
    "#'~/GitHub/BigData/dataset11'\n",
    "# import train and test dataset\n",
    "train = pd.read_csv(f'{dataset_directory}/{dataset_train_file}',header=None)\n",
    "test = pd.read_csv(f'{dataset_directory}/{dataset_test_file}',header=None)\n",
    "# Import dataset\n",
    "X_train = train.iloc[:,1:].apply(lambda a: a*(0.99/255)+0.01)\n",
    "y_train = train.iloc[:,:1]\n",
    "X_test = test.iloc[:,1:].apply(lambda a: a*(0.99/255)+0.01)\n",
    "y_test = test.iloc[:,:1]\n",
    "# The class labels are integer values, encoding them into one-hot representation https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(categories=\"auto\", sparse=False, dtype=np.int16) # handle_unknown='ignore'\n",
    "y_train = enc.fit_transform(y_train.to_numpy().reshape(-1, 1))\n",
    "y_test = enc.fit_transform(y_test.to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "Rescale the data to values between 0.01 and 1, by multiplying each pixel value in the dataset by (0.99/255) and adding 0.01 to the result to get a new pixel value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(X_train, y_train, config):\n",
    "    n_input = config[0]\n",
    "    n_output = config[1]\n",
    "    n_nodes = config[2]\n",
    "    epoch = config[3]\n",
    "    batch = config[4]\n",
    "    # Define and Compile\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_nodes, input_dim=n_input, activation='relu'))\n",
    "    model.add(Dense(n_output, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train, epochs=epoch, batch_size=batch)\n",
    "    return model\n",
    "\n",
    "# predict with the fit model\n",
    "def model_predict(model, testdata):\n",
    "    Prediction = model.predict(testdata, verbose=0)\n",
    "    return Prediction\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = 784 # Input features\n",
    "outputs = 10 # Output features\n",
    "nodes = [5,10,15,20,35,50] # Test amount of nodes in layer\n",
    "\n",
    "configs = []\n",
    "for n in nodes:\n",
    "    config_ = [inputs, outputs, n, 75, 250]\n",
    "    configs.append(config_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kfolding\n",
    "config_lossess = []\n",
    "accuracy_per_config = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "counter = 0\n",
    "for config in configs:\n",
    "    counter+=1\n",
    "    fold_count = 0\n",
    "\n",
    "    fold_losses = []\n",
    "    fold_accuracy = []\n",
    "    \n",
    "    for fold_train, fold_test in kf.split(X_train, y_train):\n",
    "        fold_count+=1\n",
    "        nn_model = model_fit(X_train[fold_train], y_train[fold_train], config)\n",
    "        scores = neural_network_model.evaluate(X_train[fold_test], y_train[fold_test], verbose=0)\n",
    "        print(f\"Score for config {counter} fold {fold_count}: {neural_network_model.metrics_names[0]}={scores[0]}; {neural_network_model.metrics_names[1]}={scores[1]*100}%\")\n",
    "        \n",
    "        fold_losses.append(scores[0])\n",
    "        fold_accuracy.append(scores[1]*100)\n",
    "\n",
    "    config_lossess.append(fold_losses)\n",
    "    accuracy_per_config.append(fold_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score per config: \")\n",
    "for j in range(0, len(accuracy_per_config)):\n",
    "    print('^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^')\n",
    "    print(f'Score for config {j+1}. Nodes:{configs[j][2]}')\n",
    "    print('Average scores for all folds:')\n",
    "    print(f'> Loss: {np.mean(config_lossess[j])}')\n",
    "    print(f'> Accuracy: {np.mean(accuracy_per_config[j])} (+- {np.std(accuracy_per_config[j])})')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "configy = [784, 10, 20, 10, 1]\n",
    "neural_network = model_fit(X_train, y_train, configy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "prediction = model_predict(neural_network, X_test)\n",
    "\n",
    "PredClass = list()\n",
    "ActualClass = list()\n",
    "for row in range(len(y_test)):\n",
    "    index_maxscore1 = max(range(len(y_test[row])), key=y_test[row].__getitem__)\n",
    "    ActualClass.append(index_maxscore1)\n",
    "    index_maxscore2 = max(range(len(prediction[row])), key=prediction[row].__getitem__)\n",
    "    PredClass.append(index_maxscore2)\n",
    "    print('Expected=%d, Got=%d' % (index_maxscore1, index_maxscore2))\n",
    "\n",
    "\n",
    "accuracy = accuracy_metric(PredClass, ActualClass)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    import tensorflow as tf\n",
    "    if tf.test.gpu_device_name():\n",
    "        print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "    else:\n",
    "        print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}